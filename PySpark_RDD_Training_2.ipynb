{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "702bbbb7-d163-468b-beda-d79347c7cbec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: ['1,101,10,150.00,2025-01-01',\n '2,102,3,50.00,2025-01-02',\n '3,103,7,200.00,2025-01-03',\n '4,104,6,120.00,2025-01-04',\n '5,105,2,80.00,2025-01-05']"
     ]
    }
   ],
   "source": [
    " #How can you filter sales transactions where the sale_amount is greater than 100.00 and the quantity_sold is greater than 5.\n",
    " Sales_RDD=sc.textFile('dbfs:/FileStore/Sales.csv')\n",
    " header=Sales_RDD.first()\n",
    " Sales_RDD_Filter=Sales_RDD.filter(lambda x:x!=header)\n",
    " Sales_RDD_Filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff23639-c1c9-4e59-93fe-ca6d4c100e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: [['1', '101', '10', '150.00', '2025-01-01'],\n ['3', '103', '7', '200.00', '2025-01-03'],\n ['4', '104', '6', '120.00', '2025-01-04']]"
     ]
    }
   ],
   "source": [
    "Sales_RDD_Filter_Result=Sales_RDD_Filter.map(lambda x:x.split(',')).filter(lambda x:float(x[3])>100.00 and int(x[2])>5)\n",
    "Sales_RDD_Filter_Result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0af954-cb42-40ee-a28a-5237775c4962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_customers_rdd.map(lambda x: float(x[4])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a22c358-ebed-4079-b605-28180f623c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: [('A', 7568),\n ('J', 5913),\n ('M', 4637),\n ('C', 3699),\n ('L', 3479),\n ('E', 3440),\n ('S', 3362),\n ('B', 2112),\n ('K', 2076),\n ('D', 2068)]"
     ]
    }
   ],
   "source": [
    "#Problem Statement:\n",
    "#Find the top 5 by count of starting letter of the name from Baby_Names dataset Show the starting letter of the name with count arranged in the descending order of count\n",
    "baby_names = sc.textFile('dbfs:/FileStore/Baby_Names.csv').map(lambda x:x.split(',')).map(lambda arr:arr[1]).groupBy(lambda x:x[0]).map(lambda t:(t[0],len(t[1]))).sortBy(lambda a:a[1],ascending=False)\n",
    "baby_names.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ecc6fe2-3e57-4499-8f18-a69ad1928751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [('is', 4),\n ('execution', 2),\n ('framework', 2),\n ('run', 2),\n ('hadoop', 3),\n ('scala', 3),\n ('preferred', 2),\n ('language', 2),\n ('supports', 2),\n ('java', 3)]"
     ]
    }
   ],
   "source": [
    "#Count the number of times a word is repeated in all sentences\n",
    "wordcount = sc.textFile('dbfs:/FileStore/wordcount.txt')\n",
    "wordcount = wordcount.flatMap(lambda line: line.split(\" \"))  \\\n",
    "                .map(lambda word: (word, 1)) \\\n",
    "                .reduceByKey(lambda a, b: a + b)\n",
    "wordcount.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96357cd5-56c6-4559-8353-3432b587a191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: ['spark',\n 'is',\n 'a',\n 'general',\n 'purpose',\n 'execution',\n 'framework',\n 'spark',\n 'can',\n 'run',\n 'on',\n 'hadoop',\n 'scala',\n 'is',\n 'preferred',\n 'language',\n 'for',\n 'spark',\n 'spark',\n 'also',\n 'supports',\n 'java',\n 'and',\n 'python',\n 'spark',\n 'is',\n 'a',\n 'general',\n 'purpose',\n 'execution',\n 'framework',\n 'spark',\n 'can',\n 'run',\n 'on',\n 'hadoop',\n 'scala',\n 'is',\n 'preferred',\n 'language',\n 'for',\n 'spark',\n 'spark',\n 'also',\n 'supports',\n 'java',\n 'and',\n 'python',\n 'python',\n 'spark',\n 'scala',\n 'java',\n 'pyspark',\n 'hadoop',\n 'spark',\n 'python',\n 'spark',\n 'rdd',\n 'rdd',\n 'rdd',\n 'sql',\n 'spark',\n 'pythom',\n 'machine',\n 'learning',\n 'spark',\n 'sql',\n 'rdd',\n 'rdd']"
     ]
    }
   ],
   "source": [
    "wordcount = sc.textFile('dbfs:/FileStore/wordcount.txt')\n",
    "wordcount = wordcount.flatMap(lambda line: line.split(\" \")) \n",
    "wordcount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387badd1-51cd-477e-8f57-768ee50c6d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 10)\n"
     ]
    }
   ],
   "source": [
    "seqOp = (lambda x, y:(x[0]+y[0],x[1]+y[1]))\n",
    "combOp = (lambda x, y:(x[0] + y[0],x[1]+y[1]))\n",
    "rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "rdd2=rdd1.map(lambda x:(x,1))\n",
    "sumCount = rdd2.aggregate((0,0),seqOp, combOp)\n",
    "print(sumCount)\n",
    "#reduce method can be used in solving above method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29fe6eb-0ee0-4888-a963-184037d9e690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hadoop', 1), ('pyspark', 2), ('python', 2), ('spark', 2)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"python\", 12), (\"pyspark\", 11), (\"spark\", 11), (\"hadoop\", 11), (\"python\", 12), (\"pyspark\", 1), (\"spark\", 1)])\n",
    "out1 = sorted(rdd.countByKey().items())\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36b5857-12ef-4611-b10b-00980459f8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('hadoop', 1), 1), (('pyspark', 1), 1), (('pyspark', 2), 1), (('python', 1), 2), (('spark', 1), 2)]\n[(1, 1), (2, 4), (3, 3), (4, 3), (5, 1), (6, 2), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"python\", 1), (\"pyspark\", 2), (\"spark\", 1), (\"hadoop\", 1), (\"python\", 1), (\"pyspark\", 1), (\"spark\", 1)])\n",
    "out1 = sorted(rdd.countByValue().items())\n",
    "print(out1)\n",
    "rdd2 = sc.parallelize([1,2,2,2,3,4,2,4,5,3,6,7,8,4,6,3])\n",
    "out2 = sorted(rdd2.countByValue().items())\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eaa122f-9716-453c-9376-107d397a6519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "out1 = sc.parallelize([10, 2, 9, 3, 4, 5, 6, 7,1]).takeOrdered(6)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e88e183-d3a3-40a4-bf95-1ccbb2fc3c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 9, 3, 1, 0, 0, 6, 8, 3, 6, 9, 9, 2, 1, 0, 1, 2, 2, 4, 9]\n[9, 6, 2, 0, 8]\n[1, 5, 6, 0, 9, 4, 7, 2, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(0, 10))\n",
    "sample1 = rdd.takeSample(True, 20, 1)\n",
    "print(sample1)\n",
    "\n",
    "sample2 = rdd.takeSample(False, 5,4)\n",
    "print(sample2)\n",
    "\n",
    "sample3 = rdd.takeSample(False, 15, 3)\n",
    "print(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af04fb1a-a592-4815-a3a8-ebde62a2fe7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: [[1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]"
     ]
    }
   ],
   "source": [
    "rdd_parallal = sc.parallelize(range(1, 12), 3)  #(1,2,3), (4,5,6,7), (8,9,10,11)\n",
    "\n",
    "rdd_parallal.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150b81ec-a163-49f1-965d-5b96c0de9139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', [1, 3, 5]), ('key2', [2, 4, 7])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd1 = sc.parallelize([(\"key1\", 1), (\"key2\", 2), (\"key1\", 3)])       \n",
    "rdd2 = sc.parallelize([(\"key1\", 5), (\"key2\", 4), (\"key2\", 7)])\n",
    "\n",
    "grp = rdd1.cogroup(rdd2).mapValues(lambda val: [i for e in val for i in e])\n",
    "print(grp.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3cd347f-76de-40da-be5f-0dec470e40cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 2]), ('b', [1])]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "def to_list(a):\n",
    "\treturn [a]\n",
    "\n",
    "def append(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "\n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "out = sorted(x.combineByKey(to_list, append, extend).collect())\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351ccf62-e0eb-4e77-ba06-27dda88882ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('harsha', 1), ('aditya', 1), ('raju', 1), ('veer', 1)]\n[('amrita', 1), ('raju', 1), ('veer', 1), ('anita', 1)]\n[('raju', (1, 1)), ('veer', (1, 1))]\n[('raju', (1, 1)), ('harsha', (1, None)), ('aditya', (1, None)), ('veer', (1, 1))]\n[('amrita', (None, 1)), ('raju', (1, 1)), ('anita', (None, 1)), ('veer', (1, 1))]\n[('amrita', (None, 1)), ('raju', (1, 1)), ('anita', (None, 1)), ('harsha', (1, None)), ('aditya', (1, None)), ('veer', (1, 1))]\n"
     ]
    }
   ],
   "source": [
    "names1 = sc.parallelize([\"harsha\", \"aditya\", \"raju\", \"veer\"]).map(lambda a: (a, 1))\n",
    "names2 = sc.parallelize([\"amrita\", \"raju\", \"veer\", \"anita\"]).map(lambda a: (a, 1))\n",
    "\n",
    "print( names1.collect())\n",
    "print( names2.collect())\n",
    "\n",
    "join = names1.join(names2)\n",
    "print( join.collect() )\n",
    "\n",
    "leftOuterJoin = names1.leftOuterJoin(names2)\n",
    "print( leftOuterJoin.collect() )\n",
    "\n",
    "rightOuterJoin = names1.rightOuterJoin(names2)\n",
    "print( rightOuterJoin.collect() )\n",
    "\n",
    "fullOuterJoin = names1.fullOuterJoin(names2)\n",
    "print( fullOuterJoin.collect() )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_RDD_Training_2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}