{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0e46eb-3cec-4f94-b9b3-6c36b4ccc9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|department| Total|\n+----------+------+\n|        IT|233000|\n|        HR|122000|\n|   Finance| 90000|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#You are given a dataset of employees in an Indian company with columns: `emp_id`, `name`, `department`, `salary`, and `city`. Write a PySpark program to find the total salary paid in each department.\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import desc,col\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "dataFrame1 = spark.read.csv('dbfs:/FileStore/psday1_1.csv', header=True, schema=schema)\n",
    "result = dataFrame1.groupBy('department').agg({'salary':'sum'})#.sum('salary')\n",
    "result = result.withColumnRenamed('sum(salary)', 'Total')\n",
    "result = result.withColumn('Total', col('Total').cast('int')).orderBy(desc('Total'))\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8196c431-889e-4074-afab-94d62edb992a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+---------+----------+---------------+\n|product_category|cust_id|cust_name|      city|purchase_amount|\n+----------------+-------+---------+----------+---------------+\n|     Electronics|    201|     Aman|     Delhi|         1500.0|\n|         Fashion|    202|    Kiran|    Mumbai|         2000.0|\n|     Electronics|    203|     Ravi| Bangalore|         2000.0|\n|         Fashion|    204|   Simran| Hyderabad|         2000.0|\n|     Electronics|    205|    Vinay|      Pune|         1800.0|\n|         Grocery|    206|    Pooja|   Chennai|         1300.0|\n+----------------+-------+---------+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#You are given a dataset containing customer transactions in an Indian e-commerce platform with columns: `cust_id`, `cust_name`, `city`, `purchase_amount`, and `product_category`. Some records have missing `purchase_amount`. Write a PySpark program to fill missing `purchase_amount` values with the average purchase amount of that product category.\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col, round,when\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('cust_id', IntegerType(), True),\n",
    "    StructField('cust_name', StringType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('purchase_amount', FloatType(), True),\n",
    "    StructField('product_category', StringType(), True)\n",
    "])\n",
    "\n",
    "Dataframe2 = spark.read.csv('dbfs:/FileStore/psday1_2.csv', header=True, schema=schema)\n",
    "result = Dataframe2.groupBy('product_category').agg(round(avg('purchase_amount'),2).alias('Cat_Avg'))\n",
    "joined_df=Dataframe2.join(result,on='product_category',how='left')\n",
    "filled_df = joined_df.withColumn('purchase_amount', when(col('purchase_amount').isNull(), col('Cat_Avg')).otherwise(col('purchase_amount'))).drop('Cat_Avg')\n",
    "filled_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335e58eb-1ed9-4d4f-a64d-47a1146b36a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-----+----+\n|student_id|student_name|       state|score|rank|\n+----------+------------+------------+-----+----+\n|       302|       Sneha|   Karnataka| 92.0|   1|\n|       304|       Kunal|   Karnataka| 88.0|   2|\n|       306|       Pavan|   Karnataka| 80.0|   3|\n|       303|        Amit| Maharashtra| 90.0|   1|\n|       301|       Rohit| Maharashtra| 85.0|   2|\n|       305|       Nidhi| Maharashtra| 78.0|   3|\n+----------+------------+------------+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "#You have a dataset of students from different Indian states with columns: `student_id`, `student_name`, `state`, `score`. Write a PySpark program to rank students within each state based on their scores in descending order.\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, desc, rank\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('student_id', IntegerType(), True),\n",
    "    StructField('student_name', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('score', FloatType(), True)\n",
    "])\n",
    "\n",
    "Dataframe3=spark.read.csv('dbfs:/FileStore/psday2_1.csv',header=True,schema=schema)\n",
    "window_spec=Window.partitionBy('state').orderBy(desc('score'))\n",
    "ranked_df = Dataframe3.withColumn(\"rank\", rank().over(window_spec))#.filter('rank==1')\n",
    "ranked_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abc3409-263d-4989-9d16-260870c86ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n|   name|department|salary|rank|\n+-------+----------+------+----+\n|Charlie|      \"HR\"|7000.0|   1|\n|    Bob|      \"HR\"|6000.0|   2|\n|  Frank|      \"IT\"|9000.0|   1|\n|  David|      \"IT\"|8000.0|   2|\n+-------+----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data = [\n",
    "#(\"Alice\", \"HR\", 5000),\n",
    "#(\"Bob\", \"HR\", 6000),\n",
    "#(\"Charlie\", \"HR\", 7000),\n",
    "#(\"David\", \"IT\", 8000),\n",
    "#(\"Eve\", \"IT\", 7500),\n",
    "#(\"Frank\", \"IT\", 9000)\n",
    "#]\n",
    "#columns = [\"name\", \"department\", \"salary\"]\n",
    "#Dataframe4=spark.createDataFrame(data,columns)\n",
    "#Dataframe4.show()\n",
    "#Find the top 2 highest salaries in each department\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "schema=StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('department',StringType(),True),\n",
    "    StructField('salary',FloatType(),True)\n",
    "])\n",
    "Dataframe4=spark.read.csv('dbfs:/FileStore/psday2_2.csv',header=True,schema=schema)\n",
    "window_spec=Window.partitionBy('department').orderBy(desc('salary'))\n",
    "Ranked_df=Dataframe4.withColumn('rank',dense_rank().over(window_spec)).where('rank<=2')\n",
    "Ranked_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1d71af-251e-4997-a569-51ced3a21d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n|user_id|total_spent|\n+-------+-----------+\n|   C002|     1500.0|\n|   C003|     1000.0|\n|   C001|      800.0|\n+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#You have a PySpark DataFrame containing customer transaction details. Write a PySpark query to find the top 3 customers who have spent the most money, sorted in descending order of total spend. \n",
    "from pyspark.sql.types import StringType,StructField,StructType,FloatType\n",
    "from pyspark.sql.functions import sum\n",
    "schema=StructType([\n",
    "    StructField(\"user_id\",StringType(),True),\n",
    "    StructField(\"amount\",FloatType(),True)\n",
    "])\n",
    "DataFrame5=spark.read.csv('dbfs:/FileStore/psday3_1.csv',header=True,schema=schema)\n",
    "#result1=DataFrame5.groupBy(\"user_id\").agg(sum(\"amount\").alias(\"total_spent\"))\n",
    "result=DataFrame5.groupBy(\"user_id\").agg(sum(\"amount\").alias(\"total_spent\")).orderBy(\"total_spent\",ascending=False).limit(3)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fcf12b2-5ea5-4f9f-92fa-d3f7425bff52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n|user_id|        first_login|         last_login|\n+-------+-------------------+-------------------+\n|   U002|2024-03-10 09:30:00|2024-03-10 14:00:00|\n|   U003|2024-03-10 11:15:00|2024-03-10 22:00:00|\n|   U001|2024-03-10 08:00:00|2024-03-10 18:30:00|\n+-------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#You have a PySpark DataFrame containing user login details. Write a PySpark query to find the first and last login timestamps for each user.\n",
    "from pyspark.sql.types import StructField, StructType, TimestampType, StringType\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('user_id', StringType(), True),\n",
    "    StructField('timestamp', TimestampType(), True)\n",
    "])\n",
    "\n",
    "DataFrame6 = spark.read.csv('dbfs:/FileStore/psday3_2.csv', header=True, schema=schema)\n",
    "\n",
    "result = DataFrame6.groupBy(\"user_id\").agg(\n",
    "    min(\"timestamp\").alias(\"first_login\"),\n",
    "    max(\"timestamp\").alias(\"last_login\")\n",
    ")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad76340-46ee-48bc-a06a-0ae12b6db9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+------------+\n|customer_id|order_date|amount|total_amount|\n+-----------+----------+------+------------+\n|        101|2024-02-01| 700.0|      1400.0|\n|        103|2024-03-15| 400.0|       500.0|\n|        102|2024-01-01| 300.0|       300.0|\n|        104|2024-03-10| 100.0|       100.0|\n+-----------+----------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#For each customer, return the latest order amount, the total amount spent, and the number of orders. Output should include the customer's name.\n",
    "\n",
    "from pyspark.sql.functions import to_date, desc, rank,col,sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('Customer_name', StringType(), True)\n",
    "])\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField('order_id', IntegerType(), True),\n",
    "    StructField('Customer_id', IntegerType(), True),\n",
    "    StructField('order_date', StringType(), True),\n",
    "    StructField('amount', FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "DataFrame7_1 = spark.read.csv('dbfs:/FileStore/psday4_1.csv', header=True, schema=schema1)\n",
    "DataFrame7_2 = spark.read.csv('dbfs:/FileStore/psday4_2.csv', header=True, schema=schema2)\n",
    "\n",
    "\n",
    "DataFrame7_2 = DataFrame7_2.withColumn(\"order_date\", to_date(DataFrame7_2[\"order_date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "\n",
    "window_spec = Window.partitionBy('customer_id').orderBy(desc('order_date'))\n",
    "\n",
    "\n",
    "\n",
    "result = DataFrame7_2.withColumn('ranker', rank().over(window_spec)).filter(col('ranker') == 1)\n",
    "\n",
    "result_agg=DataFrame7_2.groupBy('customer_id').agg(sum('amount').alias('total_amount'))\n",
    "final_result=result.join(result_agg,on='customer_id').select('customer_id','order_date','amount','total_amount')\n",
    "final_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204e5657-efe8-4cce-be1a-66ba73514b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------+\n|employee_id|employee_name|dept_id|dept_name|\n+-----------+-------------+-------+---------+\n|          1|        Alice|    101|     'HR'|\n|          2|          Bob|    102|'Finance'|\n|          3|      Charlie|    103|     'IT'|\n|          4|        David|    101|     'HR'|\n+-----------+-------------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#You are given two DataFrames in PySpark:\n",
    "#employee_df: Contains employee information.\n",
    "#department_df: Contains department information.\n",
    "#You need to perform an inner join on these DataFrames to find out which department each employee belongs to\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType\n",
    "schema1=StructType(\n",
    "    [StructField('employee_id',IntegerType(),True),\n",
    "     StructField('employee_name',StringType(),True),\n",
    "     StructField('dept_id',IntegerType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "schema2=StructType(\n",
    "    [StructField('dept_id',IntegerType(),True),\n",
    "     StructField('dept_name',StringType(),True),\n",
    "    ]\n",
    ")\n",
    "Dataframe8_1=spark.read.csv('dbfs:/FileStore/psday4_2_1.csv',header=True,schema=schema1)\n",
    "Dataframe8_2=spark.read.csv('dbfs:/FileStore/psday4_2_2.csv',header=True,schema=schema2)\n",
    "result=Dataframe8_1.join(Dataframe8_2,on='dept_id').select('employee_id','employee_name','dept_id','dept_name')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed0475c-2160-4f18-baa4-c32678544f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>skillset</th><th>2020-11-01</th><th>2020-11-02</th></tr></thead><tbody><tr><td>B</td><td>2</td><td>0</td></tr><tr><td>A</td><td>1</td><td>1</td></tr><tr><td>S</td><td>0</td><td>2</td></tr><tr><td>R</td><td>1</td><td>0</td></tr><tr><td>I</td><td>1</td><td>0</td></tr><tr><td>H</td><td>0</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B",
         2,
         0
        ],
        [
         "A",
         1,
         1
        ],
        [
         "S",
         0,
         2
        ],
        [
         "R",
         1,
         0
        ],
        [
         "I",
         1,
         0
        ],
        [
         "H",
         0,
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "skillset",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2020-11-01",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "2020-11-02",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#You have data with skillset(list) and date.now you have count the people with number of skills with respective dates\n",
    "from pyspark.sql.functions import explode,to_date\n",
    "\n",
    "data=[(['A','B'],'01/11/20'),\n",
    "(['B','I','R'],'01/11/20'),\n",
    "(['S','H'],'02/11/20'),\n",
    "(['A','H','S'],'02/11/20')]\n",
    "\n",
    "columns=['skillset','date']\n",
    "\n",
    "Dataframe8_1=spark.createDataFrame(data,columns)\n",
    "Dataframe8_1_1=Dataframe8_1.withColumn('dates',to_date(Dataframe8_1['date'],'dd/MM/yy'))\n",
    "result=Dataframe8_1_1.withColumn('skillset',explode('skillset'))\n",
    "grouped_df=result.groupBy('skillset','dates').count()\n",
    "pivot_df=grouped_df.groupBy('skillset').pivot('dates',['2020-11-01', '2020-11-02']).sum('count')\n",
    "final_result=pivot_df.fillna(0)\n",
    "display(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0db8c0-b0e4-46b8-8179-c04767a1f0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>department</th><th>salary</th><th>ranker</th><th>Avg_Sal</th></tr></thead><tbody><tr><td>7</td><td>'FINANCE'</td><td>65000.0</td><td>1</td><td>64000.0</td></tr><tr><td>8</td><td>'FINANCE'</td><td>63000.0</td><td>2</td><td>64000.0</td></tr><tr><td>6</td><td>'HR'</td><td>71000.0</td><td>1</td><td>60333.33</td></tr><tr><td>2</td><td>'HR'</td><td>60000.0</td><td>2</td><td>60333.33</td></tr><tr><td>9</td><td>'IT'</td><td>90000.0</td><td>1</td><td>69250.0</td></tr><tr><td>5</td><td>'IT'</td><td>72000.0</td><td>2</td><td>69250.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         "'FINANCE'",
         65000.0,
         1,
         64000.0
        ],
        [
         8,
         "'FINANCE'",
         63000.0,
         2,
         64000.0
        ],
        [
         6,
         "'HR'",
         71000.0,
         1,
         60333.33
        ],
        [
         2,
         "'HR'",
         60000.0,
         2,
         60333.33
        ],
        [
         9,
         "'IT'",
         90000.0,
         1,
         69250.0
        ],
        [
         5,
         "'IT'",
         72000.0,
         2,
         69250.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "ranker",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Avg_Sal",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#You are given a DataFrame of employee data with the following columns:\n",
    "#employee_id (int)\n",
    "#department (string)\n",
    "#salary (int)\n",
    "#Write PySpark code to:\n",
    "#Rank employees within each department based on their salary (highest salary first).\n",
    "#Add a column that shows the average salary per department.\n",
    "#Filter to keep only the top 2 highest-paid employees per department.\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,desc,avg,col,round\n",
    "schema=StructType([\n",
    "    StructField('employee_id',IntegerType(),True),\n",
    "    StructField('department',StringType(),True),\n",
    "    StructField('salary',FloatType(),True)\n",
    "])\n",
    "\n",
    "Dataframe8_2=spark.read.csv('dbfs:/FileStore/psday5_2.csv',header=True,schema=schema)\n",
    "windows_spec=Window.partitionBy('department').orderBy(desc('salary'))\n",
    "avg_window=Window.partitionBy('department')\n",
    "result1=Dataframe8_2.withColumn('ranker',rank().over(windows_spec)).withColumn('Avg_Sal',round(avg('salary').over(avg_window),2))\n",
    "final_result=result1.filter(col('ranker')<= 2)\n",
    "display(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a869b7c-bd51-46be-8531-38edd754998c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>skills</th></tr></thead><tbody><tr><td>1</td><td>spark</td></tr><tr><td>1</td><td>hadoop</td></tr><tr><td>1</td><td>hive</td></tr><tr><td>2</td><td>python</td></tr><tr><td>2</td><td> flask</td></tr><tr><td>3</td><td>sql</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "spark"
        ],
        [
         1,
         "hadoop"
        ],
        [
         1,
         "hive"
        ],
        [
         2,
         "python"
        ],
        [
         2,
         " flask"
        ],
        [
         3,
         "sql"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "skills",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#You are given a PySpark DataFrame with a column named tags that contains comma-separated string\n",
    "#Task: Write PySpark code to transform this DataFrame so that each tag appears in its own row, along with the corresponding id,\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "Data = [\n",
    "    (1, \"spark,hadoop,hive\"),\n",
    "    (2, \"python, flask\"),\n",
    "    (3, \"sql\")\n",
    "]\n",
    "\n",
    "col_names = ['id', 'skillset']\n",
    "DataFrame9_1 = spark.createDataFrame(Data, col_names)\n",
    "\n",
    "result = DataFrame9_1.withColumn('Skills', explode(split(col('skillset'),\",\"))).select('id','skills')\n",
    "display(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c6d544f-f450-4fb1-8618-32875f550ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>city</th><th>country</th></tr></thead><tbody><tr><td>'John'</td><td>28</td><td>'New York'</td><td>'USA'</td></tr><tr><td>'Sarah'</td><td>24</td><td>'London'</td><td>'UK'</td></tr><tr><td>'Micheal'</td><td>30</td><td>'Sydney'</td><td>'Australia'</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "'John'",
         28,
         "'New York'",
         "'USA'"
        ],
        [
         "'Sarah'",
         24,
         "'London'",
         "'UK'"
        ],
        [
         "'Micheal'",
         30,
         "'Sydney'",
         "'Australia'"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#How would you add a new column called Country to a PySpark DataFrame based on the values of an existing City column, using different methods in PySpark?\n",
    "from pyspark.sql.types import StructField,StructType,StringType,IntegerType\n",
    "schema=StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('age',IntegerType(),True),\n",
    "    StructField('city',StringType(),True)\n",
    "])\n",
    "schema2=StructType([\n",
    "    StructField('city',StringType(),True),\n",
    "    StructField('country',StringType(),True)\n",
    "])\n",
    "Dataframe9_2_1=spark.read.csv('dbfs:/FileStore/psday6_1.csv',header=True,schema=schema)\n",
    "Dataframe9_2_2=spark.read.csv('dbfs:/FileStore/psday6_1_2.csv',header=True,schema=schema2)\n",
    "result=Dataframe9_2_1.join(Dataframe9_2_2,on='city').select('name','age','city','country')\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a38c3b61-742b-4dcd-b473-c452aae54331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>city</th><th>country</th></tr></thead><tbody><tr><td>John</td><td>28</td><td>New York</td><td>USA</td></tr><tr><td>Sarah</td><td>24</td><td>London</td><td>UK</td></tr><tr><td>Micheal</td><td>30</td><td>Sydney</td><td>Australia</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         28,
         "New York",
         "USA"
        ],
        [
         "Sarah",
         24,
         "London",
         "UK"
        ],
        [
         "Micheal",
         30,
         "Sydney",
         "Australia"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import StructField,StructType,StringType,IntegerType\n",
    "schema=StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('age',IntegerType(),True),\n",
    "    StructField('city',StringType(),True)\n",
    "])\n",
    "\n",
    "\n",
    "mapping_data = [(\"New York\", \"USA\"), (\"London\", \"UK\"), (\"Sydney\", \"Australia\")]\n",
    "mapping_df = spark.createDataFrame(mapping_data, [\"City\", \"Country\"])\n",
    "Dataframe9_2_1=spark.read.csv('dbfs:/FileStore/psday6_1.csv',header=True,schema=schema)\n",
    "\n",
    "\n",
    "df_with_country = Dataframe9_2_1.join(broadcast(mapping_df), on=\"City\", how=\"left\").select('name','age','city','country')\n",
    "display(df_with_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e3fe21-6368-4a11-9301-20c62ced8b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept</th><th>count</th></tr></thead><tbody><tr><td>HR</td><td>3</td></tr><tr><td>IT</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HR",
         3
        ],
        [
         "IT",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code to count number of employees in each department.\n",
    "from pyspark.sql.types import StructField,StructType,StringType\n",
    "schema=StructType([\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('dept',StringType(),True)\n",
    "])\n",
    "\n",
    "DataFrame9_3=spark.read.csv('dbfs:/FileStore/psday6_2.csv',header=True,schema=schema)\n",
    "result=DataFrame9_3.groupBy('dept').count()\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f524a9f8-5c21-4942-877a-0a9c2eeaaedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept</th><th>Avg_sal</th></tr></thead><tbody><tr><td>Sales</td><td>60000.0</td></tr><tr><td>HR</td><td>76500.0</td></tr><tr><td>IT</td><td>55000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sales",
         60000.0
        ],
        [
         "HR",
         76500.0
        ],
        [
         "IT",
         55000.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Avg_sal",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Given employee data, filter out everyone earning ≤50K, then group by department to calculate the average salary.\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col,avg\n",
    "\n",
    "schema=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('dept',StringType(),True),\n",
    "    StructField('Salary',IntegerType(),True)\n",
    "])\n",
    "\n",
    "DataFrame10_1=spark.read.csv('dbfs:/FileStore/psday7_1.csv',header=True,schema=schema)\n",
    "Result=DataFrame10_1.filter(col('Salary')>50000).groupby('dept').agg(avg('Salary').alias('Avg_sal'))\n",
    "display(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b523212-f15d-4807-a770-9b610da8a5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dept</th><th>Salary</th><th>ranked</th></tr></thead><tbody><tr><td>7</td><td>Alice</td><td>HR</td><td>80000</td><td>1</td></tr><tr><td>2</td><td>Bob</td><td>Sales</td><td>48000</td><td>1</td></tr><tr><td>8</td><td>Charlie</td><td>IT</td><td>85000</td><td>1</td></tr><tr><td>4</td><td>Diana</td><td>Sales</td><td>60000</td><td>1</td></tr><tr><td>5</td><td>Evan</td><td>IT</td><td>49000</td><td>1</td></tr><tr><td>6</td><td>Fiona</td><td>HR</td><td>81000</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         "Alice",
         "HR",
         80000,
         1
        ],
        [
         2,
         "Bob",
         "Sales",
         48000,
         1
        ],
        [
         8,
         "Charlie",
         "IT",
         85000,
         1
        ],
        [
         4,
         "Diana",
         "Sales",
         60000,
         1
        ],
        [
         5,
         "Evan",
         "IT",
         49000,
         1
        ],
        [
         6,
         "Fiona",
         "HR",
         81000,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "ranked",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Remove duplicate rows based on name and department\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc,rank,col\n",
    "\n",
    "schema=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('dept',StringType(),True),\n",
    "    StructField('Salary',IntegerType(),True)\n",
    "])\n",
    "\n",
    "DataFrame10_2=spark.read.csv('dbfs:/FileStore/psday7_2.csv',header=True,schema=schema)\n",
    "window_spec=Window.partitionBy('name','dept').orderBy(desc('id'))\n",
    "result=DataFrame10_2.withColumn('ranked',rank().over(window_spec)).filter(col('ranked')==1)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd12c1d1-6074-482e-b64f-187b75300918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dept</th><th>Salary</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>HR</td><td>72000</td><td>7200.0</td></tr><tr><td>2</td><td>Bob</td><td>Sales</td><td>48000</td><td>4800.0</td></tr><tr><td>3</td><td>Charlie</td><td>IT</td><td>55000</td><td>5500.0</td></tr><tr><td>4</td><td>Diana</td><td>Sales</td><td>60000</td><td>6000.0</td></tr><tr><td>5</td><td>Evan</td><td>IT</td><td>49000</td><td>4900.0</td></tr><tr><td>6</td><td>Fiona</td><td>HR</td><td>81000</td><td>8100.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "HR",
         72000,
         7200.0
        ],
        [
         2,
         "Bob",
         "Sales",
         48000,
         4800.0
        ],
        [
         3,
         "Charlie",
         "IT",
         55000,
         5500.0
        ],
        [
         4,
         "Diana",
         "Sales",
         60000,
         6000.0
        ],
        [
         5,
         "Evan",
         "IT",
         49000,
         4900.0
        ],
        [
         6,
         "Fiona",
         "HR",
         81000,
         8100.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 𝑪𝒓𝒆𝒂𝒕𝒆 𝒂 𝑷𝒚𝑺𝒑𝒂𝒓𝒌 𝑫𝒂𝒕𝒂𝑭𝒓𝒂𝒎𝒆 𝒘𝒊𝒕𝒉 𝒂 “𝒃𝒐𝒏𝒖𝒔” 𝑪𝒐𝒍𝒖𝒎𝒏 (10% 𝒐𝒇 𝑺𝒂𝒍𝒂𝒓𝒚)\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "schema=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('dept',StringType(),True),\n",
    "    StructField('Salary',IntegerType(),True)\n",
    "])\n",
    "\n",
    "DataFrame10_3=spark.read.csv('dbfs:/FileStore/psday7_3.csv',header=True,schema=schema)\n",
    "result=DataFrame10_3.withColumn('bonus',col('Salary')*0.10)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff494cc3-d413-435e-9ecc-f187141def46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept</th><th>id</th><th>name</th><th>age</th></tr></thead><tbody><tr><td>HR</td><td>1</td><td>Alice</td><td>25.0</td></tr><tr><td>Sales</td><td>2</td><td>Bob</td><td>30.0</td></tr><tr><td>IT</td><td>3</td><td>Charlie</td><td>24.0</td></tr><tr><td>Sales</td><td>4</td><td>Diana</td><td>30.0</td></tr><tr><td>IT</td><td>5</td><td>Evan</td><td>24.0</td></tr><tr><td>HR</td><td>6</td><td>Fiona</td><td>25.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HR",
         1,
         "Alice",
         25.0
        ],
        [
         "Sales",
         2,
         "Bob",
         30.0
        ],
        [
         "IT",
         3,
         "Charlie",
         24.0
        ],
        [
         "Sales",
         4,
         "Diana",
         30.0
        ],
        [
         "IT",
         5,
         "Evan",
         24.0
        ],
        [
         "HR",
         6,
         "Fiona",
         25.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dept</th><th>age</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>HR</td><td>25.0</td></tr><tr><td>6</td><td>Fiona</td><td>HR</td><td>25.0</td></tr><tr><td>3</td><td>Charlie</td><td>IT</td><td>24.0</td></tr><tr><td>5</td><td>Evan</td><td>IT</td><td>24.0</td></tr><tr><td>2</td><td>Bob</td><td>Sales</td><td>30.0</td></tr><tr><td>4</td><td>Diana</td><td>Sales</td><td>30.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "HR",
         25.0
        ],
        [
         6,
         "Fiona",
         "HR",
         25.0
        ],
        [
         3,
         "Charlie",
         "IT",
         24.0
        ],
        [
         5,
         "Evan",
         "IT",
         24.0
        ],
        [
         2,
         "Bob",
         "Sales",
         30.0
        ],
        [
         4,
         "Diana",
         "Sales",
         30.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Given a Dataframe with some null values in age column now you have replace it with average values\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col,avg,coalesce\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "schema=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('dept',StringType(),True),\n",
    "    StructField('age',IntegerType(),True)\n",
    "])\n",
    "\n",
    "DataFrame10_4=spark.read.csv('dbfs:/FileStore/pyday7_4.csv',header=True,schema=schema)\n",
    "result=DataFrame10_4.groupBy('dept').agg(avg('age').alias('Avg_age'))\n",
    "joined_df=DataFrame10_4.join(result,on='dept')\n",
    "Final_result = joined_df.withColumn(\"age\", coalesce(col(\"age\"), col(\"Avg_age\"))).drop('Avg_age')\n",
    "display(Final_result)\n",
    "#second\n",
    "\n",
    "dept_window = Window.partitionBy(\"dept\")\n",
    "Final_result1= DataFrame10_4.withColumn(\n",
    "    \"age\",\n",
    "    coalesce(col(\"age\"), avg(\"age\").over(dept_window))\n",
    ")\n",
    "\n",
    "display(Final_result1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151ea60f-fca2-401a-a0ca-913082364451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>team_id</th><th>team_name</th><th>points</th></tr></thead><tbody><tr><td>10</td><td>leetcode FC</td><td>7</td></tr><tr><td>50</td><td>Tornto FC</td><td>3</td></tr><tr><td>20</td><td>Newyork FC</td><td>3</td></tr><tr><td>30</td><td>Atlanta FC</td><td>1</td></tr><tr><td>40</td><td>Chicago FC</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "leetcode FC",
         7
        ],
        [
         50,
         "Tornto FC",
         3
        ],
        [
         20,
         "Newyork FC",
         3
        ],
        [
         30,
         "Atlanta FC",
         1
        ],
        [
         40,
         "Chicago FC",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "team_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "team_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "points",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Write a solution that selects the team_id, team_name and num_points of each team in the tournament after all described matches. Return the result table ordered by num_points in decreasing order. In case of a tie, order the records by team_id in increasing order.\n",
    "#You would like to compute the scores of all teams after all matches. Points are awarded as follows:\n",
    "# A team receives three points if they win a match (i.e., Scored more goals than the opponent team).\n",
    "# A team receives one point if they draw a match (i.e., Scored the same number of goals as the opponent team).\n",
    "# A team receives no points if they lose a match (i.e., Scored fewer goals than the opponent team).\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col,when,sum,desc\n",
    "\n",
    "schema1=StructType([\n",
    "    StructField('team_id',IntegerType(),True),\n",
    "    StructField('team_name',StringType(),True),\n",
    "    \n",
    "])\n",
    "\n",
    "schema2=StructType([\n",
    "    StructField('match_id',IntegerType(),True),\n",
    "    StructField('host_team',IntegerType(),True),\n",
    "    StructField('guest_team',IntegerType(),True),\n",
    "    StructField('host_goals',IntegerType(),True),\n",
    "    StructField('guest_goals',IntegerType(),True)\n",
    "])\n",
    "\n",
    "Dataframe11_1_1=spark.read.csv('dbfs:/FileStore/pyday8_11.csv',header=True,schema=schema1)\n",
    "Dataframe11_1_2=spark.read.csv('dbfs:/FileStore/psday8_12.csv',header=True,schema=schema2)\n",
    "\n",
    "result=Dataframe11_1_1.join(Dataframe11_1_2,(Dataframe11_1_1.team_id==Dataframe11_1_2.host_team)|(Dataframe11_1_1.team_id==Dataframe11_1_2.guest_team),how='left')\n",
    "\n",
    "final_result=result.withColumn('points', \n",
    "    when (\n",
    "        (col('team_id')==col('host_team')) & (col('host_goals')>col('guest_goals')),3\n",
    "        ).\n",
    "    when(\n",
    "        (col('team_id')==col('guest_team')) & (col('host_goals')<col('guest_goals')),3\n",
    "        ).\n",
    "    when(\n",
    "        (col('host_goals')==col('guest_goals')),1\n",
    "        ).\n",
    "    otherwise(0))\n",
    "final_result_1=final_result.groupBy('team_id','team_name').agg(sum('points').alias('points')).orderBy(desc('points'))\n",
    "\n",
    "display(final_result_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae166fe5-8935-4b18-884b-d66e9ed92672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>Salary</th><th>dept_id</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>70000.0</td><td>10</td></tr><tr><td>3</td><td>Charlie</td><td>80000.0</td><td>10</td></tr><tr><td>6</td><td>Frank</td><td>90000.0</td><td>10</td></tr><tr><td>2</td><td>Bob</td><td>69333.33</td><td>20</td></tr><tr><td>5</td><td>Eve</td><td>75000.0</td><td>20</td></tr><tr><td>8</td><td>Hannah</td><td>62000.0</td><td>20</td></tr><tr><td>10</td><td>Jack</td><td>71000.0</td><td>20</td></tr><tr><td>4</td><td>David</td><td>52000.0</td><td>30</td></tr><tr><td>7</td><td>Grace</td><td>52000.0</td><td>30</td></tr><tr><td>9</td><td>Isaac</td><td>52000.0</td><td>30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         70000.0,
         10
        ],
        [
         3,
         "Charlie",
         80000.0,
         10
        ],
        [
         6,
         "Frank",
         90000.0,
         10
        ],
        [
         2,
         "Bob",
         69333.33,
         20
        ],
        [
         5,
         "Eve",
         75000.0,
         20
        ],
        [
         8,
         "Hannah",
         62000.0,
         20
        ],
        [
         10,
         "Jack",
         71000.0,
         20
        ],
        [
         4,
         "David",
         52000.0,
         30
        ],
        [
         7,
         "Grace",
         52000.0,
         30
        ],
        [
         9,
         "Isaac",
         52000.0,
         30
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Problem Statement:\n",
    "#You're given two DataFrames:\n",
    "#employees – contains employee details:\n",
    "#(employee_id, name, salary, department_id)\n",
    "#departments – contains department metadata:\n",
    "#(department_id, department_name)\n",
    "#🎯 Objective:\n",
    "#Fill in the missing (null) salaries in the employees DataFrame with the average salary of that respective department.\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col,avg,coalesce,round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "schema1=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('salary',IntegerType(),True),\n",
    "    StructField('dept_id',IntegerType(),True)\n",
    "])\n",
    "\n",
    "schema2=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('dept_name',StringType(),True)\n",
    "])\n",
    "\n",
    "\n",
    "Dataframe11_2_1=spark.read.csv('dbfs:/FileStore/psday8_21.csv',header=True,schema=schema1)\n",
    "Dataframe11_2_2=spark.read.csv('dbfs:/FileStore/psday8_22.csv',header=True,schema=schema2)\n",
    "windows_spec=Window.partitionBy('dept_id')\n",
    "Final_result= Dataframe11_2_1.withColumn(\n",
    "    \"Salary\",\n",
    "    coalesce(col(\"Salary\"), round(avg(\"Salary\").over(windows_spec),2))\n",
    ")\n",
    "display(Final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06f39b4-1967-43c9-b1a4-4c6c8ece7d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>manager_name</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>null</td></tr><tr><td>2</td><td>jane</td><td>John</td></tr><tr><td>3</td><td>Sam</td><td>John</td></tr><tr><td>4</td><td>Lucy</td><td>jane</td></tr><tr><td>5</td><td>Mike</td><td>Sam</td></tr><tr><td>6</td><td>Tyson</td><td>jane</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         null
        ],
        [
         2,
         "jane",
         "John"
        ],
        [
         3,
         "Sam",
         "John"
        ],
        [
         4,
         "Lucy",
         "jane"
        ],
        [
         5,
         "Mike",
         "Sam"
        ],
        [
         6,
         "Tyson",
         "jane"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "manager_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Write pyspark code to display manager of each employees\n",
    "#How do you retrieve the manager’s name for each employee using PySpark?\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import col\n",
    "schema=StructType([\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('deptid',IntegerType(),True),\n",
    "    StructField('mgr_id',IntegerType(),True)\n",
    "])\n",
    "\n",
    "Dataframe12_1=spark.read.csv('dbfs:/FileStore/psday9_1.csv',header=True,schema=schema)\n",
    "result = Dataframe12_1.alias('employee') .join(Dataframe12_1.alias('manager'), col('employee.mgr_id') == col('manager.id'), 'left').select(col('employee.id'), col('employee.name'), col('manager.name').alias('manager_name'))\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c7df2b-ca3d-429b-bdd9-8d91d4ed4e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>acnt_id</th></tr></thead><tbody><tr><td>A001</td></tr><tr><td>A004</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A001"
        ],
        [
         "A004"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "acnt_id",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Identify accounts that should be banned based on logins from different IP addresses at the same time.\n",
    "from pyspark.sql.functions import to_timestamp,col,desc,lead\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "schema = StructType([\n",
    "    StructField('acnt_id', StringType(), True),\n",
    "    StructField('ip_address', StringType(), True),\n",
    "    StructField('login', StringType(), True),\n",
    "    StructField('logout', StringType(), True)\n",
    "])\n",
    "Dataframe12_2= spark.read.csv('dbfs:/FileStore/psday9_2.csv', header=True, schema=schema)\n",
    "Dataframe12_2= Dataframe12_2.withColumn('login', to_timestamp('login', 'yy-MM-dd HH:mm:ss')) \\\n",
    "       .withColumn('logout', to_timestamp('logout', 'yy-MM-dd HH:mm:ss'))\n",
    "window_spec=Window.partitionBy('acnt_id').orderBy('login')\n",
    "Dup_Dataframe=Dataframe12_2.withColumn('next_login',lead('login').over(window_spec)).filter(col('next_login').isNotNull())\n",
    "final_df=Dup_Dataframe.filter(col('logout')>col('next_login')).select('acnt_id').distinct()\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83872ebb-c3e4-4fa7-bf01-1d8be17f9376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Descr</th><th>count</th></tr></thead><tbody><tr><td>TV</td><td>5</td></tr><tr><td>Chair</td><td>3</td></tr><tr><td>Sofa</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "TV",
         5
        ],
        [
         "Chair",
         3
        ],
        [
         "Sofa",
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Descr",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "from pyspark.sql.functions import explode,col,split\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('Descr', StringType(), True)\n",
    "])\n",
    "Dataframe13_1=spark.read.csv('dbfs:/FileStore/psday10_1-2.csv',header=True,schema=schema)\n",
    "result=Dataframe13_1.withColumn('Descr',split(col('Descr'),',')).withColumn('Descr',explode('Descr'))\n",
    "final_result=result.groupBy('Descr').count()\n",
    "display(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7b86c8-6521-40b2-96f7-100ad87337fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>consecutiveNums</th></tr></thead><tbody><tr><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "consecutiveNums",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#write a pyspark code to identify the consecutive numbers from the Dataframe\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,lead,lag\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('num', IntegerType(), True)\n",
    "])\n",
    "Dataframe13_2=spark.read.csv('dbfs:/FileStore/psday10_2.csv',header=True,schema=schema)\n",
    "window_spec=Window.orderBy('id')\n",
    "Dataframe13_2_1=Dataframe13_2.withColumn('prev',lag('num').over(window_spec)).withColumn('next',lead('num').over(window_spec))\n",
    "final_result=Dataframe13_2_1.filter((col('num')==col('prev')) & (col('num')==col('next'))).select(col('num').alias('consecutiveNums'))\n",
    "display(final_result)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 477155367163430,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_INPRAC",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}